{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3LevpX0f4id"
      },
      "source": [
        "# Danny Silvestre Assignment 3\n",
        "Computational Foundations of AI\n",
        "12/3/2021\n",
        "z23431842\n",
        "Professor Michael DeGiorgio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDHVr6XHa_lW"
      },
      "source": [
        "### This Module prepares the data and initializes an organized, scaled dataset. (CreditData)\n",
        "# importing the module\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# read specific columns of csv file using Pandas \n",
        "Features = []\n",
        "Labels = []\n",
        "df = pd.read_csv(\"TrainingData_N183_p10.csv\")\n",
        "#Feature Extraction\n",
        "featureExtract = pd.read_csv(\"TrainingData_N183_p10.csv\",usecols = [\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"PC5\",\"PC6\",\"PC7\",\"PC8\",\"PC9\",\"PC10\"])\n",
        "\n",
        "quantitative = [\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"PC5\",\"PC6\",\"PC7\",\"PC8\",\"PC9\",\"PC10\"]\n",
        "\n",
        "\n",
        "for featureName in featureExtract:\n",
        "  featureCol = []\n",
        "  for featureValues in featureExtract[featureName]:\n",
        "    featureCol.append(featureValues)\n",
        "  Features.append(featureCol)\n",
        "\n",
        "#Feature Normalization and Scaling\n",
        "ScaledFeatures = []\n",
        "for featureType in Features:\n",
        "  print(featureType)\n",
        "  scaledFeatureType = []\n",
        "  mean = np.mean(featureType)\n",
        "  std = np.std(featureType)\n",
        "  #Scale each feature value\n",
        "  for val in featureType:\n",
        "    newVal = (val - mean) / std\n",
        "    scaledFeatureType.append(newVal)\n",
        "  ScaledFeatures.append(scaledFeatureType)\n",
        "#Labels in list.\n",
        "for label in df['Ancestry']:\n",
        "  Labels.append(label)\n",
        "\n",
        "\n",
        "print(\"Features\")\n",
        "print(Features)\n",
        "print(\"Scaled Features\")\n",
        "print(np.array(ScaledFeatures).shape)\n",
        "print(\"Training Data\")\n",
        "print(featureExtract)\n",
        "\n",
        "X = [[ScaledFeatures[0][i]] + [ScaledFeatures[1][i]] + [ScaledFeatures[2][i]] + [ScaledFeatures[3][i]] + [ScaledFeatures[4][i]] + [ScaledFeatures[5][i]] + [ScaledFeatures[6][i]] + [ScaledFeatures[7][i]] + [ScaledFeatures[8][i]] + [ScaledFeatures[9][i] ]for i in range(len(ScaledFeatures[0]))]\n",
        "\n",
        "F = np.array(X)\n",
        "print(F.shape)\n",
        "print(X[0])\n",
        "# Centering labels\n",
        "#Labels -= np.mean(Labels)\n",
        "y = Labels\n",
        "\n",
        "#ùêæ =5 \n",
        "#continental  ancestries  (African,  European,  East  Asian,  Oceanian,  or  Native  American).\n",
        "cat = [\"African\",\"European\",\"EastAsian\", \"Oceanian\", \"NativeAmerican\"] \n",
        "def one_hot_encoding(labels,categories):\n",
        "  newArray = []\n",
        "  for item in labels:\n",
        "    tmp = np.zeros(len(categories))\n",
        "    for x in range(len(categories)):\n",
        "      if item == categories[x]:\n",
        "        tmp[x] = 1\n",
        "    newArray.append(tmp)\n",
        "  return newArray\n",
        "y = one_hot_encoding(y,cat)\n",
        "print(np.array(y).shape)\n",
        "#print(df)\n",
        "#printing the first instance of credit data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-tyC0hoB5vC"
      },
      "source": [
        "# Centering labels\n",
        "#Labels -= np.mean(Labels)\n",
        "#y = Labels\n",
        "#y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdYivS7yB51o"
      },
      "source": [
        "import sys\n",
        "\n",
        "from random import seed\n",
        "from random import randrange\n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(x,y, folds):\n",
        "\tfeatures = []\n",
        "\tlabels = []\n",
        "\tx_copy = list(x)\n",
        "\ty_copy = list(y)\n",
        "\tfold_size = int(len(x) / folds)\n",
        "\tfor i in range(folds):\n",
        "\t\tx_fold = []\n",
        "\t\ty_fold = []\n",
        "\t\twhile len(x_fold) < fold_size:\n",
        "\t\t\tindex = randrange(len(x_copy))\n",
        "\t\t\tx_fold.append(x_copy.pop(index))\n",
        "\t\t\ty_fold.append(y_copy.pop(index))\n",
        "\t\tfeatures.append(x_fold)\n",
        "\t\tlabels.append(y_fold)\n",
        "\treturn features,labels\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKOv1bbvB53v"
      },
      "source": [
        "import time\n",
        "from progressbar import ProgressBar\n",
        "class RidgeRegression() :\n",
        "      \n",
        "    def __init__( self, learning_rate, iterations, tuning,verbose ) :\n",
        "      if verbose == 1:\n",
        "        print(\"Initializing Model with learning rate of \" + str(learning_rate))\n",
        "        print(\"Number of Iterations \" + str(iterations))        \n",
        "        print(\"Tuning Parameter \" + str(tuning)) \n",
        "      self.learning_rate = learning_rate\n",
        "      self.tuning = tuning\n",
        "      self.iterations = iterations\n",
        "      self.verbose = verbose \n",
        "    # This is going to be used to train the models           \n",
        "    def fit( self, X, Y ) :\n",
        "          \n",
        "        # This is the number of training examples and number of features stored in the class.       \n",
        "        self.numExamples, self.numFeatures = X.shape\n",
        "        if (self.verbose >= 1):\n",
        "          print(\"Calculating number of training examples...\")\n",
        "          print(self.numExamples)\n",
        "          print(\"Calculating number of features...\")\n",
        "          print(self.numFeatures)\n",
        "          if (self.verbose == 2):\n",
        "            print(\"Current Weights\")\n",
        "            print(np.rand( self.numFeatures ))\n",
        "        # step 3 inializing random weights     \n",
        "        self.W = np.random.rand(self.numFeatures)\n",
        "        # Initializing the output layer because there are 5 classes. One for each type of ancestry\n",
        "        self.outputLayer = np.random.rand(5)\n",
        "        self.b = 0        \n",
        "        self.X = X        \n",
        "        self.Y = Y\n",
        "        #This gives it time so it doesnt overwrite the other print functions\n",
        "        time.sleep(0.5)\n",
        "        # GDL\n",
        "        #Progress bar is used so that the console stays organized rather than printing out multiple lines.\n",
        "        pbar = ProgressBar()\n",
        "        print(\"\\n iterations\")\n",
        "        for i in pbar(range(self.iterations )):        \n",
        "            self.update_weights()            \n",
        "        return self\n",
        "      \n",
        "    # Updates weights for GDL\n",
        "    def update_weights( self ) :        \n",
        "        Y_pred = self.predict( self.X )\n",
        "\n",
        "        # calculating the gradients  \n",
        "        #print(\"self.x.t\") \n",
        "        #print(self.X.T) \n",
        "        #Change in the weights  \n",
        "        totalLoss = self.Y - Y_pred\n",
        "        class1Loss = [totalLoss[i][0] for i in range(len(self.X))]\n",
        "        class2Loss = [totalLoss[i][1] for i in range(len(self.X))]\n",
        "        class3Loss = [totalLoss[i][2] for i in range(len(self.X))]\n",
        "        class4Loss = [totalLoss[i][3] for i in range(len(self.X))]\n",
        "        class5Loss = [totalLoss[i][4] for i in range(len(self.X))]\n",
        "        totalLoss = [class1Loss,class2Loss,class3Loss,class4Loss,class5Loss]\n",
        "        dWEnd = 0\n",
        "        dBEnd = 0\n",
        "        for loss in totalLoss:\n",
        "          dW = ( - ( 2 * ( self.X.T ).dot( loss) ) +               \n",
        "               ( 2 * self.tuning * self.W ) ) / self.numExamples \n",
        "        #change in b     \n",
        "          db = - 2 * np.sum( loss ) / self.numExamples\n",
        "          dWEnd += dW\n",
        "          dBEnd += db\n",
        "        dW = dWEnd\n",
        "        db = dBEnd\n",
        "        # update weights    \n",
        "        if (verbose == 2):\n",
        "          print(\"Updating weights...\")\n",
        "        self.W = self.W - self.learning_rate * dW \n",
        "        self.b = self.b - self.learning_rate * db     \n",
        "        if (verbose == 2 ):\n",
        "          print(\"new weights: \")\n",
        "          print(self.W)\n",
        "          print(\"new b value: \")\n",
        "          print(self.b)      \n",
        "        return self\n",
        "      \n",
        "    \n",
        "    def predict( self, X ) :\n",
        "      predictions = []\n",
        "      for ex in X:\n",
        "        prediction = [ex.dot(self.W) * self.outputLayer[0],   ex.dot(self.W) * self.outputLayer[0],ex.dot(self.W) * self.outputLayer[0],ex.dot(self.W) * self.outputLayer[0],ex.dot(self.W) * self.outputLayer[0] ]\n",
        "        predictions.append(prediction)\n",
        "      return predictions\n",
        "      #return X.dot( self.W ) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsodI3BPURSk"
      },
      "source": [
        "#9 Features plot of coefficients.\n",
        "Tuning_parameters = [10**-2,10**-1,10**0,10**1,10**2,10**3,10**4]\n",
        "lr = 10**-5\n",
        "iteration = 10**4\n",
        "verbose = 0\n",
        "coef = []\n",
        "#Africancoef = []\n",
        "#Europeancoef = []\n",
        "#EastAsiancoef = []\n",
        "#Oceaniancoef = []\n",
        "#NativeAmericancoef = []\n",
        "tX = np.array(X)\n",
        "#African = [y[i][0] for i in range(len(y))]\n",
        "#European = [y[i][1] for i in range(len(y))]\n",
        "#EastAsian = [y[i][2] for i in range(len(y))]\n",
        "#Oceanian = [y[i][3] for i in range(len(y))]\n",
        "#NativeAmerican = [y[i][4] for i in range(len(y))]\n",
        "\n",
        "tY = np.array(y)\n",
        "#tY = [African,European,EastAsian,Oceanian,NativeAmerican]\n",
        "#tY = np.array(tY)\n",
        "for tuningParameter in Tuning_parameters:\n",
        "  ridge = RidgeRegression(lr,iteration,tuningParameter,verbose)\n",
        "  ridge.fit(tX,tY)\n",
        "  coef.append(ridge.W)\n",
        "print(coef)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ60yu7KSP_D"
      },
      "source": [
        "#Deliverable 1\n",
        "import matplotlib.pyplot as plt\n",
        "pltfeatures = [\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"PC5\",\"PC6\",\"PC7\",\"PC8\",\"PC9\",\"PC10\"]\n",
        "PC1= []\n",
        "PC2 = []\n",
        "PC3 = []\n",
        "PC4 = []\n",
        "PC5 = []\n",
        "PC6 = []\n",
        "PC7 = []\n",
        "PC8 = []\n",
        "PC9 = []\n",
        "PC10 = []\n",
        "for i in range(0,7):\n",
        "  PC1.append(coef[i][0])\n",
        "  PC2.append(coef[i][1])\n",
        "  PC3.append(coef[i][2])\n",
        "  PC4.append(coef[i][3])\n",
        "  PC5.append(coef[i][4])\n",
        "  PC6.append(coef[i][5])\n",
        "  PC7.append(coef[i][6])\n",
        "  PC8.append(coef[i][7])\n",
        "  PC9.append(coef[i][8])\n",
        "  PC10.append(coef[i][9])\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "plt.xscale('log')\n",
        "ax.plot(Tuning_parameters,PC1,label=\"PC1\")\n",
        "ax.plot(Tuning_parameters,PC2,label=\"PC2\")\n",
        "ax.plot(Tuning_parameters,PC3,label=\"PC3\")\n",
        "ax.plot(Tuning_parameters,PC4,label=\"PC4\")\n",
        "ax.plot(Tuning_parameters,PC5,label=\"PC5\")\n",
        "ax.plot(Tuning_parameters,PC6,label=\"PC6\")\n",
        "ax.plot(Tuning_parameters,PC7,label=\"PC7\")\n",
        "ax.plot(Tuning_parameters,PC8,label=\"PC8\")\n",
        "ax.plot(Tuning_parameters,PC9,label=\"PC9\")\n",
        "ax.plot(Tuning_parameters,PC10,label=\"PC10\")\n",
        "ax.legend(loc = 'upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtzB3vByB56V"
      },
      "source": [
        "#Training the model using Cross Validation\n",
        "seed(1)\n",
        "Tuning_parameters = [10**-2,10**-1,10**0,10**1,10**2,10**3,10**4]\n",
        "lr = 10**-5\n",
        "iteration = 10**4\n",
        "fold_num = 5\n",
        "verbose = 1\n",
        "ft,lb= cross_validation_split(X,y,5)\n",
        "\n",
        "mseResults = []\n",
        "for tuningParameter in Tuning_parameters:\n",
        "  model = RidgeRegression(lr,iteration,tuningParameter,verbose)\n",
        "  foldMSE = []\n",
        "  for i in range(0,fold_num):\n",
        "    print(\"***************************************\")\n",
        "    print(\"Fold \" + str(i) + \"/\" + str(fold_num))\n",
        "    print(\"Tuning parameter \" + str(tuningParameter) )\n",
        "    print(\"***************************************\")\n",
        "    valx = ft[i]\n",
        "    valy = lb[i]\n",
        "    trainx = []\n",
        "    trainy = []\n",
        "    for x in range (0,fold_num):\n",
        "      if x != i:\n",
        "        trainx += ft[x]\n",
        "        trainy += lb[x]\n",
        "    trainx = np.array(trainx)\n",
        "    trainy = np.array(trainy)\n",
        "    valx = np.array(valx)\n",
        "    valy = np.array(valy)\n",
        "    #print(len(trainx))\n",
        "    #print(len(trainy))\n",
        "    #print(len(valx))\n",
        "    #print(len(valy))\n",
        "    model.fit(trainx,trainy)\n",
        "    prediction = model.predict(valx)\n",
        "    n = len(valy)\n",
        "    #print(\"error\")\n",
        "    #print(valy - prediction)\n",
        "    squaredError = ((valy - prediction) ** 2)\n",
        "    error = np.mean(squaredError)\n",
        "    #calculate test set accuracy here\n",
        "    foldMSE.append(error)\n",
        "  print(foldMSE)\n",
        "  meanFoldMSE = np.mean(foldMSE)\n",
        "  mseResults.append(meanFoldMSE)\n",
        "print(mseResults)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAsflW8XTmNG"
      },
      "source": [
        "#Deliverable 2\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots()\n",
        "plt.xscale('log')\n",
        "print(Tuning_parameters)\n",
        "print(mseResults)\n",
        "plt.plot(Tuning_parameters, mseResults)\n",
        "ax.set_ylabel(\"MSE\")\n",
        "ax.set_xlabel('\\u03BB')\n",
        "ax.set_title(' 5 CV Tuning Parameters')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L20v5-qjCVDl"
      },
      "source": [
        "#Deliverable 3: Indicate the value of ùúÜ that generated the smallest CV(5) error. \n",
        "minCVError = min(mseResults)\n",
        "minCVIndex = mseResults.index(minCVError)\n",
        "ùúÜ = Tuning_parameters[minCVIndex]\n",
        "print(minCVError)\n",
        "print(\"optimal lamda\")\n",
        "ùúÜ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D04_c-6PRtbN"
      },
      "source": [
        "#Deliverable 4\n",
        "optimalLamda = ùúÜ\n",
        "lr = 10**-5\n",
        "iteration = 10**4\n",
        "verbose = 1\n",
        "coef = []\n",
        "tX = np.array(X)\n",
        "tY = np.array(y)\n",
        "\n",
        "ridge = RidgeRegression(lr,iteration,optimalLamda,verbose)\n",
        "ridge.fit(tX,tY)\n",
        "coef = ridge.W\n",
        "print(\"Best Params Estimate\")\n",
        "print(coef)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxB56a2dilSS"
      },
      "source": [
        "### Collecting test data\n",
        "\n",
        "\n",
        "\n",
        "featureExtract = pd.read_csv(\"TestData_N111_p10.csv\",usecols = [\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"PC5\",\"PC6\",\"PC7\",\"PC8\",\"PC9\",\"PC10\"])\n",
        "\n",
        "quantitative = [\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"PC5\",\"PC6\",\"PC7\",\"PC8\",\"PC9\",\"PC10\"]\n",
        "\n",
        "\n",
        "for featureName in featureExtract:\n",
        "  featureCol = []\n",
        "  for featureValues in featureExtract[featureName]:\n",
        "    featureCol.append(featureValues)\n",
        "  Features.append(featureCol)\n",
        "\n",
        "#Feature Normalization and Scaling\n",
        "ScaledFeatures = []\n",
        "for featureType in Features:\n",
        "  print(featureType)\n",
        "  scaledFeatureType = []\n",
        "  mean = np.mean(featureType)\n",
        "  std = np.std(featureType)\n",
        "  #Scale each feature value\n",
        "  for val in featureType:\n",
        "    newVal = (val - mean) / std\n",
        "    scaledFeatureType.append(newVal)\n",
        "  ScaledFeatures.append(scaledFeatureType)\n",
        "#Labels in list.\n",
        "for label in df['Ancestry']:\n",
        "  Labels.append(label)\n",
        "\n",
        "\n",
        "print(\"Features\")\n",
        "print(Features)\n",
        "print(\"Scaled Features\")\n",
        "print(np.array(ScaledFeatures).shape)\n",
        "print(\"Training Data\")\n",
        "print(featureExtract)\n",
        "\n",
        "X = [[ScaledFeatures[0][i]] + [ScaledFeatures[1][i]] + [ScaledFeatures[2][i]] + [ScaledFeatures[3][i]] + [ScaledFeatures[4][i]] + [ScaledFeatures[5][i]] + [ScaledFeatures[6][i]] + [ScaledFeatures[7][i]] + [ScaledFeatures[8][i]] + [ScaledFeatures[9][i] ]for i in range(len(ScaledFeatures[0]))]\n",
        "\n",
        "F = np.array(X)\n",
        "print(F.shape)\n",
        "print(X[0])\n",
        "# Centering labels\n",
        "#Labels -= np.mean(Labels)\n",
        "y = Labels\n",
        "\n",
        "#ùêæ =5 \n",
        "#continental  ancestries  (African,  European,  East  Asian,  Oceanian,  or  Native  American).\n",
        "cat = [\"African\",\"European\",\"EastAsian\", \"Oceanian\", \"NativeAmerican\"] \n",
        "def one_hot_encoding(labels,categories):\n",
        "  newArray = []\n",
        "  for item in labels:\n",
        "    tmp = np.zeros(len(categories))\n",
        "    for x in range(len(categories)):\n",
        "      if item == categories[x]:\n",
        "        tmp[x] = 1\n",
        "    newArray.append(tmp)\n",
        "  return newArray\n",
        "y = one_hot_encoding(y,cat)\n",
        "print(np.array(y).shape)\n",
        "#print(df)\n",
        "#printing the first instance of credit data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cKS6GxHivEB"
      },
      "source": [
        "### Making predictions for all of the classes\n",
        "predictions = ridge.predict(np.array(X))\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzGuEjF3jakG"
      },
      "source": [
        "#Deliverable 5\n",
        "# The unknown seem to be predicted as majority african and mexican ancestry. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ppcDpVPiTG2"
      },
      "source": [
        "!jupyter nbconvert --to html Assignment3.ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sJzweQAOc9u"
      },
      "source": [
        "# I tried to implement this assignment correctly, however I think I did not do it correctly and may have implemented a new type of neural network or something\n",
        "# because I am certain this is not the correct implementation probably. "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}